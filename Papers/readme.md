

<p align="center">
  <img src="https://github.com/mbzuai-oryx/Video-LLaVA/blob/main/docs/images/figures/teaser.png" alt="Video-LLaVA Teaser Image" width="50%" align="left" />
  <div>
    <strong>2023</strong><br/>
    <strong>PG-Video-LLaVA: Pixel Grounding Large Video-Language Models</strong><br/>
    Shehan Munasinghe*, <strong>Rusiru Thushara*</strong>, Muhammad Maaz, Hanoona Rasheed, Salman Khan, Mubarak Shah, Fahad S. Khan<br/>
    Extends image-based LLMs to videos understanding, incorporating audio transcripts for enhanced context understanding, introducing a baseline framework and benchmark for conversation-driven spatial grounding. * - equal contribution<br/><br/>

    Arxiv preprint: <a href="https://arxiv.org/pdf/2311.13435.pdf">https://arxiv.org/pdf/2311.13435.pdf</a><br/>
    Project page: <a href="https://mbzuai-oryx.github.io/Video-LLaVA/">https://mbzuai-oryx.github.io/Video-LLaVA/</a><br/>
    Github: <a href="https://github.com/mbzuai-oryx/Video-LLaVA">https://github.com/mbzuai-oryx/Video-LLaVA</a><br/>
  </div>
</p>
<div style="clear:both;"></div>
