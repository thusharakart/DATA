

<p align="center">
  <img src="https://github.com/mbzuai-oryx/Video-LLaVA/blob/main/docs/images/figures/teaser.png" alt="Video-LLaVA Teaser Image" width="50%" align="left" />
  <div>
    <strong>2023</strong><br/>
    <strong>PG-Video-LLaVA: Pixel Grounding Large Video-Language Models</strong><br/>
    Shehan Munasinghe*, <strong>Rusiru Thushara*</strong>, Muhammad Maaz, Hanoona Rasheed, Salman Khan, Mubarak Shah, Fahad S. Khan<br/>
    <br/>
    Extends image-based LLMs to videos understanding, incorporating audio transcripts for enhanced context understanding, introducing a baseline framework and benchmark for conversation-driven spatial grounding.<br/>
    <br/>
    Arxiv preprint: https://arxiv.org/pdf/2311.13435.pdf<br/>
    Project page: https://mbzuai-oryx.github.io/Video-LLaVA/<br/>
    Github: https://github.com/mbzuai-oryx/Video-LLaVA<br/>
    <br/><br/>
    * equal contribution<br/>

  </div>
</p>
<div style="clear:both;"></div>
